---
layout: project
permalink: /:title/
category: projects

meta:
  keywords: "ROS"

project:
  title: "Robot Jenga Assistant"
  type: "Jekyll"
  url: "https://github.com/ME495-EmbeddedSystems/hw3group-jengabells"
  logo: "/assets/images/projects/jenga/pulling_crop.png"
  tech: "ROS 2, Computer Vision, MoveIt"

7pieces: ST-r9X2dZMg
10pieces: pXBVbCDGRMg
---



<p>In this project, my group and I created a ROS2 package to turn an Franka Emika Panda 7DOF arm into a Jenga assistant. Our core goal was to have the robot play alongside a human. The user would select a block and partially remove it from the tower. The robot would then detect this block, remove it the rest of the way, and place it on the top of the tower.</p> 

<br>

{% include youtubePlayer.html id=page.10pieces %}

<br>

### Computer Vision

My main role in this project was to implement the perception pipeline in order to detect where the Jenga tower and the partially removed pieces are. We used an Intel Realsense D435i camera, which provides an RGB-D image, and placed it directly above the center of the tower, looking down. From this viewpoint, there are three groups of objects that exist at distinct distances from the camera: first, the top of the tower; second, any partially removed pieces; and finally, the table. 

<br>

Upon startup, the CV node performs a scan through the depth frame to locate the top of the tower and the table. Then, it repeatedly scans between these two locations to search for pieces to pull. When one is found, the node broadcasts the location of its centroid with respect to the camera, so that the movement node can start planning to grab the piece. The transformation between camera frame and robot frame is handled by a separate calibration run using April Tags. 

<br>

![Description](/assets/images/projects/jenga/scanning_cropped.gif)
<center><h2>Early work scanning to detect the top of the tower, a piece, and the table within a square region surrounding the tower. </h2></center>


<!-- <br>

![Description](/assets/images/projects/jenga/contours2.png)
<center><h2>Detecting a piece using the depth frame information. The large blue box desginates the border, anything outside of that boundary is disregarded. The green outlines are contours of large area at this particular "slice" of the depth frame. I choose the largest contour as the piece. The red point is the centroid of this largest contour, and where the arm will grab. </h2></center> -->

<br>

The vision node also computes the orientation of the top of the tower on its initial scan. This is necessary so that the pulled pieces can be placed in the correct orientation. When it locates the top of the tower in the depth frame, it applies Canny edge detection and Hough lines on the corresponding portion of the RGB image to determine the angle of the pieces.

<br>

![Description](/assets/images/projects/jenga/edges.png)
<center><h2>Detecting the orientation of the tower top</h2></center>

<br>


### Motion

The block's starting location is computed during the CV scans, and the desired ending locations are precomputed based on the initial height/orientation of the tower and the width of the pieces (so as to ensure that the 3 pieces placed per layer are adjacent). We followed a motion plan that first orients the end effector to the piece, then moves in a cartesian path to grab it and pull it slowly straight backwards. Then, we raise the piece up, orient it to the correct ending location, place it slightly offset from the top of the tower (so that the grippers don't push any other pieces on the level), and gently push it in using the tip of the grippers. All of the trajectories were planned and executed using a custom API we wrote for MoveIt2.

<br>

![Description](/assets/images/projects/jenga/fullmotion_2x.gif)
<center><h2>A full turn (2x speed)</h2></center>


<br>

### Extra Features

We added a feature to "poke" out a block from the side of the tower, using a poker we created wth a flexible silicone tip. This allows the arm to complete an entire turn without any human interaction with the tower. However, the functionality is still very limited as we chose the location of the block to poke ahead of time. Given more time with this project, it would be interesting to set up another camera to the side of the tower to detect pieces to poke automatically.

<br>

![Description](/assets/images/projects/jenga/poke_2x.gif)
<center><h2>"Poke" feature (2x speed)</h2></center>

<br>

The last feature we added was a command to knock over the tower!

<br>


![Description](/assets/images/projects/jenga/destroy_2x.gif)
<center><h2>Destroying the tower in a fit of rage<br>(2x speed)</h2></center>

<br>

### Acknowledgements
Thanks to my amazing group members: Alyssa Chen, <a href="https://lizmetzger.github.io/portfolio/" target="_blank"><u>Liz Metzger</u></a>, and <a href="https://hang-yin.github.io/portfolio/" target="_blank"><u>Hang Yin</u></a>!


<br><br>

<!-- {% include youtubePlayer.html id=page.10pieces %} -->

